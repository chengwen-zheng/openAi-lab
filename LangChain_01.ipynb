{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMwTWnKMh4MoWyN1LGc+mUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chengwen-zheng/openAi-lab/blob/main/LangChain_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain\n",
        "%pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdxOrlfWFY8y",
        "outputId": "763f818e-c0dc-4bbb-cc3c-d7e9a8652468"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.151-py3-none-any.whl (665 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m666.0/666.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.65.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>1.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>1.3->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.151 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 多次调用完成中译英，英文文答，英译中"
      ],
      "metadata": {
        "id": "1U-j3OycE4nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import openai\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Build the Drive API client\n",
        "drive_service = build('drive', 'v3')\n",
        "file_id = '14x1k7ErttIPXRcKN53g_Ogn3zfn6AEM_'\n",
        "file = drive_service.files().get(fileId=file_id).execute()\n",
        "content = drive_service.files().get_media(fileId=file_id).execute()\n",
        "\n",
        "# Read the env config.json. set env for OPENAI_API_KEY\n",
        "with io.BytesIO(content) as f:\n",
        "    config = json.load(f)\n",
        "os.environ[\"OPENAI_API_KEY\"] = config[\"api_key\"];\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"] ;"
      ],
      "metadata": {
        "id": "5EYwaVYOFlrc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai, os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", max_tokens=2048, temperature=0.5)\n",
        "\n",
        "en_to_zh_prompt = PromptTemplate(\n",
        "    template=\"请把下面这句话翻译成英文： \\n\\n {question}?\", input_variables=[\"question\"]\n",
        ")\n",
        "\n",
        "question_prompt = PromptTemplate(\n",
        "    template = \"{english_question}\", input_variables=[\"english_question\"]\n",
        ")\n",
        "\n",
        "zh_to_cn_prompt = PromptTemplate(\n",
        "    input_variables=[\"english_answer\"],\n",
        "    template=\"请把下面这一段翻译成中文： \\n\\n{english_answer}?\",\n",
        ")\n",
        "\n",
        "\n",
        "question_translate_chain = LLMChain(llm=llm, prompt=en_to_zh_prompt, output_key=\"english_question\")\n",
        "english = question_translate_chain.run(question=\"请你作为一个机器学习的专家，介绍一下CNN的原理。\")\n",
        "print(english)\n",
        "\n",
        "qa_chain = LLMChain(llm=llm, prompt=question_prompt, output_key=\"english_answer\")\n",
        "english_answer = qa_chain.run(english_question=english)\n",
        "print(english_answer)\n",
        "\n",
        "answer_translate_chain = LLMChain(llm=llm, prompt=zh_to_cn_prompt)\n",
        "answer = answer_translate_chain.run(english_answer=english_answer)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8-fvC-zE7DO",
        "outputId": "7a8d9824-6d4d-45e3-a27f-61a369d4e880"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Please explain the principle of CNN as an expert in machine learning.\n",
            "\n",
            "\n",
            "A Convolutional Neural Network (CNN) is a type of artificial neural network that is used for image recognition and classification. It is a deep learning algorithm that uses a set of convolutional layers to extract features from an image, and then uses a fully connected layer to classify the image. The convolutional layers are used to detect patterns in the image, such as edges, shapes, and textures. The fully connected layer is then used to classify the image based on the patterns it has detected. CNNs are used in a wide range of applications, such as object detection, image segmentation, and facial recognition.\n",
            "\n",
            "\n",
            "卷积神经网络（CNN）是一种用于图像识别和分类的人工神经网络。它是一种深度学习算法，使用一组卷积层从图像中提取特征，然后使用完全连接层对图像进行分类。卷积层用于检测图像中的模式，如边缘，形状和纹理。然后使用完全连接层根据检测到的模式对图像进行分类。CNN在许多应用中使用，如物体检测，图像分割和面部识别。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用Langchain简化实现"
      ],
      "metadata": {
        "id": "yj7BnldvHm_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "chinese_qa_chain = SimpleSequentialChain(\n",
        "    chains=[question_translate_chain, qa_chain, answer_translate_chain], input_key=\"question\",\n",
        "    verbose=True)\n",
        "answer = chinese_qa_chain.run(question=\"请你作为一个机器学习的专家，介绍一下CNN的原理。\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75xG4MXqHpr7",
        "outputId": "e67d736d-32cb-43b0-c7aa-933199a1a685"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "Please introduce the principle of CNN as an expert in machine learning.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Convolutional Neural Networks (CNNs) are a type of neural network used in deep learning. They are inspired by the structure of the human brain and are used to detect patterns in large datasets. CNNs are used for image recognition, natural language processing, and various other applications in the field of machine learning. The basic principle of CNNs is to detect features in an image and then classify them. This is done by using convolutional layers which apply a filter to the input image, creating feature maps which are then used to detect patterns in the image. The layers are then connected to a fully connected layer which is used to classify the input image.\u001b[0m\n",
            "\u001b[38;5;200m\u001b[1;3m\n",
            "\n",
            "卷积神经网络（CNNs）是深度学习中使用的一种神经网络。它们受到人脑结构的启发，用于检测大型数据集中的模式。CNNs用于图像识别、自然语言处理和机器学习领域的各种其他应用。CNNs的基本原理是检测图像中的特征，然后对其进行分类。这是通过使用卷积层来实现的，它将滤波器应用于输入图像，从而创建特征映射，然后用于检测图像中的模式。然后将这些层连接到完全连接的层，用于对输入图像进行分类。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "卷积神经网络（CNNs）是深度学习中使用的一种神经网络。它们受到人脑结构的启发，用于检测大型数据集中的模式。CNNs用于图像识别、自然语言处理和机器学习领域的各种其他应用。CNNs的基本原理是检测图像中的特征，然后对其进行分类。这是通过使用卷积层来实现的，它将滤波器应用于输入图像，从而创建特征映射，然后用于检测图像中的模式。然后将这些层连接到完全连接的层，用于对输入图像进行分类。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 多步骤引入不同输入参数"
      ],
      "metadata": {
        "id": "Q0x-mZVHJ_9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "q1_prompt = PromptTemplate(\n",
        "    input_variables=[\"year1\"],\n",
        "    template=\"{year1}年的欧冠联赛的冠军是哪支球队，只说球队名称。\"\n",
        ")\n",
        "q2_prompt = PromptTemplate(\n",
        "    input_variables=[\"year2\"],\n",
        "    template=\"{year2}年的欧冠联赛的冠军是哪支球队，只说球队名称。\"\n",
        ")\n",
        "q3_prompt = PromptTemplate(\n",
        "    input_variables=[\"team1\", \"team2\"],\n",
        "    template=\"{team1}和{team2}哪只球队获得欧冠的次数多一些？\"\n",
        ")\n",
        "chain1 = LLMChain(llm=llm, prompt=q1_prompt, output_key=\"team1\")\n",
        "chain2 = LLMChain(llm=llm, prompt=q2_prompt, output_key=\"team2\")\n",
        "chain3 = LLMChain(llm=llm, prompt=q3_prompt)\n",
        "\n",
        "sequential_chain = SequentialChain(chains=[chain1, chain2, chain3], input_variables=[\"year1\", \"year2\"], verbose=True)\n",
        "answer = sequential_chain.run(year1=2000, year2=2010)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJTz7gjCKCoz",
        "outputId": "86cfe9b0-8c0a-441a-eb6f-34e3c31c57ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "拜仁慕尼黑获得欧冠的次数更多，共计8次，而法国巴黎圣日耳曼队只有6次。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 通过Langchain来完成自动撰写单元测试\n"
      ],
      "metadata": {
        "id": "PVkeRNH4LX0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "\n",
        "def write_unit_test(function_to_test, unit_test_package = \"pytest\"):\n",
        "    # 解释源代码的步骤\n",
        "    explain_code = \"\"\"\"# How to write great unit tests with {unit_test_package}\n",
        "\n",
        "    In this advanced tutorial for experts, we'll use Python 3.10 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n",
        "    ```python\n",
        "    {function_to_test}\n",
        "    ```\n",
        "\n",
        "    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
        "    - First,\"\"\"\n",
        "\n",
        "    explain_code_template = PromptTemplate(\n",
        "        input_variables=[\"unit_test_package\", \"function_to_test\"],\n",
        "        template=explain_code\n",
        "    )\n",
        "    explain_code_llm = OpenAI(model_name=\"text-davinci-002\", temperature=0.4, max_tokens=1000, \n",
        "            top_p=1, stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"])\n",
        "    explain_code_step = LLMChain(llm=explain_code_llm, prompt=explain_code_template, output_key=\"code_explaination\")\n",
        "\n",
        "\n",
        "    # 创建测试计划示例的步骤\n",
        "    test_plan = \"\"\"\n",
        "        \n",
        "    A good unit test suite should aim to:\n",
        "    - Test the function's behavior for a wide range of possible inputs\n",
        "    - Test edge cases that the author may not have foreseen\n",
        "    - Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
        "    - Be easy to read and understand, with clean code and descriptive names\n",
        "    - Be deterministic, so that the tests always pass or fail in the same way\n",
        "\n",
        "    `{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
        "\n",
        "    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
        "    -\"\"\"\n",
        "    test_plan_template = PromptTemplate(\n",
        "        input_variables=[\"unit_test_package\", \"function_to_test\", \"code_explaination\"],\n",
        "        template= explain_code + \"{code_explaination}\" + test_plan\n",
        "    )\n",
        "    test_plan_llm = OpenAI(model_name=\"text-davinci-002\", temperature=0.4, max_tokens=1000, \n",
        "            top_p=1, stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"])\n",
        "    test_plan_step = LLMChain(llm=test_plan_llm, prompt=test_plan_template, output_key=\"test_plan\")\n",
        "\n",
        "    # 撰写测试代码的步骤\n",
        "    starter_comment = \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
        "    prompt_to_generate_the_unit_test = \"\"\"\n",
        "\n",
        "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
        "```python\n",
        "import {unit_test_package}  # used for our unit tests\n",
        "\n",
        "{function_to_test}\n",
        "\n",
        "#{starter_comment}\"\"\"\n",
        "\n",
        "    unit_test_template = PromptTemplate(\n",
        "        input_variables=[\"unit_test_package\", \"function_to_test\", \"code_explaination\", \"test_plan\", \"starter_comment\"],\n",
        "        template= explain_code + \"{code_explaination}\" + test_plan + \"{test_plan}\" + prompt_to_generate_the_unit_test\n",
        "    )\n",
        "    unit_test_llm = OpenAI(model_name=\"text-davinci-002\", temperature=0.4, max_tokens=1000, stop=\"```\")\n",
        "    unit_test_step = LLMChain(llm=unit_test_llm, prompt=unit_test_template, output_key=\"unit_test\")\n",
        "\n",
        "    sequential_chain = SequentialChain(chains=[explain_code_step, test_plan_step, unit_test_step], \n",
        "                                    input_variables=[\"unit_test_package\", \"function_to_test\", \"starter_comment\"], verbose=True)\n",
        "    answer = sequential_chain.run(unit_test_package=unit_test_package, function_to_test=function_to_test, starter_comment=starter_comment)\n",
        "    return f\"\"\"#{starter_comment}\"\"\" + answer\n",
        "\n",
        "\n",
        "code = \"\"\"\n",
        "def format_time(seconds):\n",
        "    minutes, seconds = divmod(seconds, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "    if hours > 0:\n",
        "        return f\"{hours}h{minutes}min{seconds}s\"\n",
        "    elif minutes > 0:\n",
        "        return f\"{minutes}min{seconds}s\"\n",
        "    else:\n",
        "        return f\"{seconds}s\"\n",
        "\"\"\"\n",
        "\n",
        "import ast\n",
        "\n",
        "def write_unit_test_automatically(code, retry=3):\n",
        "    unit_test_code = write_unit_test(code)\n",
        "    all_code = code + unit_test_code\n",
        "    tried = 0\n",
        "    while tried < retry:\n",
        "        try:\n",
        "            ast.parse(all_code)\n",
        "            return all_code\n",
        "        except SyntaxError as e:\n",
        "            print(f\"Syntax error in generated code: {e}\")\n",
        "            all_code = code + write_unit_test(code)\n",
        "            tried += 1\n",
        "\n",
        "print(write_unit_test_automatically(code))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw70apn0LZmU",
        "outputId": "69ac0b3e-f68e-41eb-8425-a6a59ee7a911"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:WARNING! stop is not default parameter.\n",
            "                    stop was transfered to model_kwargs.\n",
            "                    Please confirm that stop is what you intended.\n",
            "WARNING:langchain.llms.openai:WARNING! stop is not default parameter.\n",
            "                    stop was transfered to model_kwargs.\n",
            "                    Please confirm that stop is what you intended.\n",
            "WARNING:langchain.llms.openai:WARNING! stop is not default parameter.\n",
            "                    stop was transfered to model_kwargs.\n",
            "                    Please confirm that stop is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "def format_time(seconds):\n",
            "    minutes, seconds = divmod(seconds, 60)\n",
            "    hours, minutes = divmod(minutes, 60)\n",
            "    if hours > 0:\n",
            "        return f\"{hours}h{minutes}min{seconds}s\"\n",
            "    elif minutes > 0:\n",
            "        return f\"{minutes}min{seconds}s\"\n",
            "    else:\n",
            "        return f\"{seconds}s\"\n",
            "#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.\n",
            "#The first element of the tuple is the name of the test case, and the second element is a list of\n",
            "#the inputs to be passed to the function under test.\n",
            "@pytest.mark.parametrize(\n",
            "    \"test_case,seconds\",\n",
            "    [\n",
            "        # Test the function's behavior for a wide range of possible inputs\n",
            "        (\"normal behavior\", [0, 1, 59, 60, 61, 3599, 3600, 3601, 3660, 5400, 5401]),\n",
            "        # Test edge cases that the author may not have foreseen\n",
            "        (\"edge cases\", [60.1, 3600.1, 5400.1]),\n",
            "    ],\n",
            ")\n",
            "def test_format_time(test_case, seconds):\n",
            "    for second in seconds:\n",
            "        # We use the pytest.raises context manager to verify that the function\n",
            "        # raises a ValueError for invalid inputs\n",
            "        if test_case == \"invalid inputs\":\n",
            "            with pytest.raises(ValueError):\n",
            "                format_time(second)\n",
            "        # For all other test cases, we simply call the function and assert that\n",
            "        # the output is as expected\n",
            "        else:\n",
            "            assert format_time(second) == f\"{second}s\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}